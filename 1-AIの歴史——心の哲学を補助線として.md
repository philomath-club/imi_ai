# 1 AIの歴史——心の哲学を補助線として

人工知能の研究の歴史。

## 1.1 ダートマス会議にはじまる

2つの学派がある。

- 記号主義: 人工知能という語の提唱，人工知能研究の主流派だった
  - ダートマス大学の数学者ジョン・マッカーシー:
    - 1956年，ダートマス会議: 米国東部ダートマス大学で2カ月間の研究会が行われた。マッカーシーが主導。
    - コンピューターに記号を機械的に操作するプログラムを与えれば，人間が行う知的な仕事をさせることができるのではないか
      - 人間が行う知的な仕事: 証明，チェス，自然言語処理など
    - 研究会を企画するに当たって，この目標を「**人工知能**」と名付けた
  - 数理論理学的発想
  - ルールベース(人がルールを作る)で，「合理的な思考」のルールを記号操作のルールとして書き出して，それを記号列にくり返し適用することで目的の結果を得る
  - 《トップダウン的，学習しない》
  - 《2000年代くらいまでは，人工知能研究というと記号主義よりのテーマが多かった》

- コネクショニズム:
  - 脳を部分的に模倣: 「脳は理性のエンジンであり魂の座」(Churchland，1995)
  - 特に神経回路の模倣(ニューラルネットワーク)
  - ボトムアップ的
  - 学習する
  - 脳全体のシミュレーションができれば知能になるだろう期待
  - ブラックボックス
  - 確率・統計
  - データ収集や学習が必要で効率は悪い

近年のAIではコネクショニズム(ニューラルネットワーク)をベースに両者のいいとこ取りをすることが多い。

## 1.2 第1次AIブーム——「1人で立てたよ！」

1960年代，最初のAIブーム: 無機質な機械が「知的」なふるまいをする。

- 記号主義: サイモンのGPS
- コネクショニズム: ローゼンブラットのパーセプトロン

### 1.2.1 サイモンのGPS

記号主義。

- ハーバート・サイモン，アレン・ニューウェル:
  - 1956年，ロジックセオリスト: ラッセル，ホワイトヘッドの『プリンピキア・マテマティカ』に対応した自動定理証明器
  - 1957年，GPS(General Problem Solver): より一般的な問題(任意のルールのパズルなど)に対応
  - 基本的にはルールで記号列を変化させたとき，記号列が取り得る状態空間を全探索するような感じ
  - 人に同じ作業をさせてみると，ある程度似た手順を踏むらしい: 「人間な合理的な思考の本質は，ルールに則って記号を操作するという点に存する」

### 1.2.2 ローゼンブラットのパーセプトロン

コネクショニズム。

《現代の1980年代くらいから先のニューラルネットワークは，情報処理のための研究であって，生の細胞とは直接かんけいなかったりする。なので，下記の生物学的な治験は，実は知らなくてもいいといえばいい。》

- (生物学的な)神経回路の知見 (p.39-40 図1-2):
  - 神経細胞とその結合からなる
  - 本体の興奮⇒軸索の電位変動⇒シナプス(すきま)に神経伝達物質が放出⇒他の細胞のレセプターが電位変動⇒樹状突起で電位変化
  - シナプスの神経伝達物質の量，レセプターの数で信号強度が調整される(重み付け)
  - 興奮性・抑制性の神経伝達物質がある
  - 重み付けは変化する(学習)
  - 他からの刺激が細胞本体に蓄積され，興奮したりしなかったりする

- 1943年，ニューロンのモデル化: 「形式ニューロン」(1940年代，マカロック，ピッツ)
  - $y=f(w_1 x_1+\cdots+w_N x_N-\theta)$
  - $N$: 入力側ニューロンの個数($i$で表す)。
  - $f$: ステップ関数。$f(z):=$ $z$が正なら$1$，負なら$0$。
  - $w_i$: 重みづけ，結合負荷(実数)
  - $x_i$: 入力信号(0 または 1)
  - $\theta$: しきい値(実数)
  - $k$: くり返し回数($k$回目の学習，または$k$番目のデータ)
- 1949年，ヘッブ，シナプスの可塑性の理論
- 1958年、フランク・ローゼンブラットがパーセプトロンを発表。単純パーセプトロン(1層)
  - 学習が可能になった
  - $w_i(k + 1) = w_i(k) + η (t(k) − y(k)) x_i$
  - $t(k)$: $k$番目のデータの正解
  - $t(k) − y(k)$: $k$回目の学習が終わった時点での誤差
  - $η$: 学習率。1回の学習が大きすぎると上手くいかないことがあるのでいい感じに調整する。
  - 線形分離可能なら学習できる
  - p.48のように複数のパーセプトロンを使って(異なる$w$の初期値を与えると)カテゴリー分けができる
- 1967年，マカロック、ピッツ、ヘッブらの理論が現実の小脳と類似していることがわかり，パーセプトロンがブームに。

## 1.3 冬の時代(1)

### 1.3.1 機械翻訳の失敗

冷戦により，ロシア語から英語への機械翻訳に予算がついた。

単純な置き換えでは，単語の多義性や文，法の知識だけでは構文を決定できない文に対応できないことがあった。

1966年，機械翻訳の予算打ち切り。

### 1.3.2 パーセプトロンの限界

1969年，マービン・ミンスキーとシーモア・パパートが単純パーセプトロンは線形分離でないカテゴリー(典型的にはXOR関数でカテゴリーをわけたようなデータ)を学習できないことを指摘。大きい画像(入力の次元が大きい画像)で問題になりやすい。画像の局所的特徴は判別できても全体的な特徴を判別できない。

- 1971年，ローゼンブラット死去。
- 1973年，ライトヒル報告書で，英国でほとんどの予算打ち切り。

### 1.4 第2次AIブーム
